{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate the error introduced by the lookup table for the **angle error**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "### Import the `lib` directory\n",
    "import pathlib\n",
    "import sys\n",
    "\n",
    "# To add the files in the lib directory to the importable packages\n",
    "repo_directory = pathlib.Path().resolve().parents[1]\n",
    "lib_module_dir = str(repo_directory.joinpath(\"lib\"))\n",
    "if lib_module_dir not in sys.path:\n",
    "    sys.path.insert(0, str(repo_directory.joinpath(\"lib\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dir = \"\"\n",
    "\n",
    "assert training_dir != \"\", (\n",
    "    \"You should pick a directory generated by a model training. These are not uploaded to repository so you need to\"\n",
    "    \" generate them yourself by running `angle_error.ipynb`. To save time, you can reduce the number of epochs.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load the **error model**, i.e., **lookup table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from error_model import ErrorModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_models_path = os.path.join(training_dir, \"error_models\")\n",
    "\n",
    "assert os.path.isdir(error_models_path) and os.listdir(error_models_path), \"No error models found\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the error model from epoch 500...\n",
      "Error model successfully load:\n",
      "<class 'error_model.ErrorModel'>\n",
      "error                    : Lidar range error\n",
      "param1                   : range\n",
      "param2                   : angle\n",
      "ndim                     : 2\n",
      "x0 boundaries            : (0.1539999991655349, 5.064000129699708)\n",
      "x1 boundaries            : (1.0023871865882938e-07, 1.2194984821886805)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "error_model_file = sorted(os.listdir(error_models_path))[-1]\n",
    "error_model_filepath = os.path.join(error_models_path, error_model_file)\n",
    "epoch_str = error_model_file.replace(\"error_model_\", \"\").replace(\".pickle\", \"\")\n",
    "\n",
    "try:\n",
    "    epoch = int(epoch_str)\n",
    "except ValueError:\n",
    "    raise ValueError(f\"A valid epoch number could not be obtained for the error model file at {error_model_filepath}\")\n",
    "\n",
    "print(f\"Loading the error model from epoch {epoch}...\")\n",
    "\n",
    "with open(error_model_filepath, \"rb\") as f:\n",
    "    error_model = pickle.load(f)\n",
    "\n",
    "print(f\"Error model successfully load:\")\n",
    "print(error_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load the **GP**\n",
    "\n",
    "This is a bit of a hacky solution. Make sure to use the same parameters and data used when training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-04 12:54:57.703094: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-09-04 12:54:57.743867: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-09-04 12:54:57.744658: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-04 12:54:58.415776: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from dual_gp_model_SVGP import DualGaussianProcessWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_file = f\"epoch {epoch}.pickle\"\n",
    "params_filepath = os.path.join(training_dir, \"params\", params_file)\n",
    "assert os.path.isfile(params_filepath), f\"Parameters file does not exist for epoch {epoch}. Choose another error model.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCALE_FACTOR: float = 100\n",
    "\n",
    "error_data = pd.read_csv(\"range_error_data.csv\")\n",
    "x_data = np.vstack([error_data[\"ranges\"], error_data[\"angles\"]]).T\n",
    "y_data = error_data[\"errors\"].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params loaded from Training at 2024-08-15 Thursday at 00.21u/params/epoch 500.pickle...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/christiaan/thesis/foresee-error-models/lib/dual_gp_model_SVGP.py:299: UserWarning: Use the model generated by the `continue_training` constructor only for making predictions.\n",
      "  warnings.warn(\"Use the model generated by the `continue_training` constructor only for making predictions.\")\n",
      "2024-09-04 12:55:10.935396: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-09-04 12:55:10.937230: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1956] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "### Use this model only to do predictions\n",
    "GP_model = DualGaussianProcessWrapper.continue_training(\n",
    "    x_data=x_data,\n",
    "    y_data=y_data,\n",
    "    params_filepath=params_filepath,\n",
    "    scale_factor=SCALE_FACTOR,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Find the errors\n",
    "\n",
    "The biggest error is most likely found at the center of each interpolation interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arrays_to_grid_arrays(x0_grid: np.ndarray, x1_grid: np.ndarray) -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Convert two arrays to two arrays that contain all the points on grid spanned by the two input arrays\"\"\"\n",
    "    x0, x1 = np.meshgrid(x0_grid, x1_grid)\n",
    "    return tuple(np.vstack((x0.flatten(), x1.flatten())))\n",
    "\n",
    "\n",
    "def get_test_points(x0_grid: np.ndarray, x1_grid: np.ndarray) -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Get all test points for a 2D grid. Three positions are tested:\n",
    "        1) between the points along parameter x0\n",
    "        2) between the points along parameter x1\n",
    "        3) between the points along parameter x0 and x1\n",
    "    \"\"\"\n",
    "    x0_grid_between, x1_grid_between = x0_grid[:-1] + np.diff(x0_grid) / 2, x1_grid[:-1] + np.diff(x1_grid) / 2\n",
    "\n",
    "    x_test_arrays = []\n",
    "    # between the x0 points\n",
    "    x_test_arrays.append(arrays_to_grid_arrays(x0_grid_between, x1_grid))\n",
    "    # between the x1 points\n",
    "    x_test_arrays.append(arrays_to_grid_arrays(x0_grid, x1_grid_between))\n",
    "    # between x0 and x1 points\n",
    "    x_test_arrays.append(arrays_to_grid_arrays(x0_grid_between, x1_grid_between))\n",
    "\n",
    "    return tuple(np.hstack(x_test_arrays))\n",
    "\n",
    "\n",
    "def split_up_GP_prediction(GP_model, x_test: np.ndarray) -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Divide the GP prediction over smaller batches to keep the CPU and memory usage limited.\"\"\"\n",
    "    GP_mean_arrays, GP_std_arrays = [], []\n",
    "\n",
    "    steps = 100\n",
    "    for i in tqdm(range(steps)):\n",
    "        x_test_part = np.array_split(x_test, steps)[i]\n",
    "        values = GP_model.fast_predict_y(x_test_part)\n",
    "        _GP_mean, _GP_var = [v.numpy().squeeze() for v in values]\n",
    "        _GP_std = np.sqrt(_GP_var)\n",
    "        GP_mean_arrays.append(_GP_mean)\n",
    "        GP_std_arrays.append(_GP_std)\n",
    "\n",
    "    return np.hstack(GP_mean_arrays), np.hstack(GP_std_arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the GP model predictions\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbe68a8137d04261bf133f7edb5d9d85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the error model predictions\n"
     ]
    }
   ],
   "source": [
    "x0_test, x1_test = get_test_points(*error_model.mean_interpolator.grid)\n",
    "x_test = np.vstack((x0_test, x1_test)).T\n",
    "\n",
    "print(\"Calculating the GP model predictions\")\n",
    "GP_mean, GP_std = split_up_GP_prediction(GP_model, x_test)\n",
    "\n",
    "print(\"Calculating the error model predictions\")\n",
    "em_mean, em_std = error_model.mean_interpolator(x_test), error_model.std_interpolator(x_test)\n",
    "\n",
    "errors_mean = np.abs(GP_mean - em_mean)\n",
    "errors_std = np.abs(GP_std - em_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Biggest errors:\n",
      "\tmean error = 3.242523377161577e-07\n",
      "\tstd error = 1.1649695285595452e-06\n",
      "Average values:\n",
      "\tmean avg. = 0.0362079515702838\n",
      "\tstd avg. = 0.007842485748157338\n"
     ]
    }
   ],
   "source": [
    "print(\"Biggest errors:\")\n",
    "print(f\"\\tmean error = {errors_mean.max()}\")\n",
    "print(f\"\\tstd error = {errors_std.max()}\")\n",
    "\n",
    "print(\"Average values:\")\n",
    "print(f\"\\tmean avg. = {GP_mean.mean()}\")\n",
    "print(f\"\\tstd avg. = {GP_std.mean()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
