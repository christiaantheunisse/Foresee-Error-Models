{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate the error introduced by the lookup table for the **angle error**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "### Import the `lib` directory\n",
    "import pathlib\n",
    "import sys\n",
    "\n",
    "# To add the files in the lib directory to the importable packages\n",
    "repo_directory = pathlib.Path().resolve().parents[1]\n",
    "lib_module_dir = str(repo_directory.joinpath(\"lib\"))\n",
    "if lib_module_dir not in sys.path:\n",
    "    sys.path.insert(0, str(repo_directory.joinpath(\"lib\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you pick a training directory from the 2D GP function and not 1D scale function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dir = \"\"\n",
    "\n",
    "assert training_dir != \"\", (\n",
    "    \"You should pick a directory generated by a model training. These are not uploaded to repository so you need to\"\n",
    "    \" generate them yourself by running `angle_error.ipynb`. To save time, you can reduce the number of epochs.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load the **error model**, i.e., **lookup table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from error_model import ErrorModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_models_path = os.path.join(training_dir, \"error_models\")\n",
    "\n",
    "assert os.path.isdir(error_models_path) and os.listdir(error_models_path), \"No error models found\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the error model from epoch 1000...\n",
      "Error model successfully load:\n",
      "<class 'error_model.ErrorModel'>\n",
      "error                    : longitudinal error rate\n",
      "param1                   : acceleration\n",
      "param2                   : velocity\n",
      "ndim                     : 2\n",
      "x0 boundaries            : (-0.25, 0.25)\n",
      "x1 boundaries            : (3.602107365926208e-05, 0.5)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "error_model_file = sorted(os.listdir(error_models_path))[-1]\n",
    "error_model_filepath = os.path.join(error_models_path, error_model_file)\n",
    "epoch_str = error_model_file.replace(\"error_model_\", \"\").replace(\".pickle\", \"\")\n",
    "\n",
    "try:\n",
    "    epoch = int(epoch_str)\n",
    "except ValueError:\n",
    "    raise ValueError(f\"A valid epoch number could not be obtained for the error model file at {error_model_filepath}\")\n",
    "\n",
    "print(f\"Loading the error model from epoch {epoch}...\")\n",
    "\n",
    "with open(error_model_filepath, \"rb\") as f:\n",
    "    error_model = pickle.load(f)\n",
    "\n",
    "print(f\"Error model successfully load:\")\n",
    "print(error_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load the **GP**\n",
    "\n",
    "This is a bit of a hacky solution. Make sure to use the same parameters and data used when training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dual_gp_model_SVGP import DualGaussianProcessWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_file = f\"epoch {epoch}.pickle\"\n",
    "params_filepath = os.path.join(training_dir, \"params\", params_file)\n",
    "assert os.path.isfile(params_filepath), f\"Parameters file does not exist for epoch {epoch}. Choose another error model.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dependencies_folder = pathlib.Path().resolve().parent.joinpath(\"dependencies\")\n",
    "VelAcc = pd.read_csv(dependencies_folder.joinpath(\"VelAcc.csv\"))\n",
    "x_data = np.vstack((VelAcc[\"accelerations\"].to_numpy(), VelAcc[\"velocities\"].to_numpy())).T\n",
    "y_data = VelAcc[\"long_dt_errors\"].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params loaded from Training at 2024-08-31 Saturday at 00.48u/params/epoch 1000.pickle...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/christiaan/thesis/foresee-error-models/lib/dual_gp_model_SVGP.py:299: UserWarning: Use the model generated by the `continue_training` constructor only for making predictions.\n",
      "  warnings.warn(\"Use the model generated by the `continue_training` constructor only for making predictions.\")\n"
     ]
    }
   ],
   "source": [
    "### Use this model only to do predictions\n",
    "GP_model = DualGaussianProcessWrapper.continue_training(\n",
    "    x_data=x_data,\n",
    "    y_data=y_data,\n",
    "    params_filepath=params_filepath,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Find the errors\n",
    "\n",
    "The biggest error is most likely found at the center of each interpolation interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arrays_to_grid_arrays(x0_grid: np.ndarray, x1_grid: np.ndarray) -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Convert two arrays to two arrays that contain all the points on grid spanned by the two input arrays\"\"\"\n",
    "    x0, x1 = np.meshgrid(x0_grid, x1_grid)\n",
    "    return tuple(np.vstack((x0.flatten(), x1.flatten())))\n",
    "\n",
    "\n",
    "def get_test_points(x0_grid: np.ndarray, x1_grid: np.ndarray) -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Get all test points for a 2D grid. Three positions are tested:\n",
    "        1) between the points along parameter x0\n",
    "        2) between the points along parameter x1\n",
    "        3) between the points along parameter x0 and x1\n",
    "    \"\"\"\n",
    "    x0_grid_between, x1_grid_between = x0_grid[:-1] + np.diff(x0_grid) / 2, x1_grid[:-1] + np.diff(x1_grid) / 2\n",
    "\n",
    "    x_test_arrays = []\n",
    "    # between the x0 points\n",
    "    x_test_arrays.append(arrays_to_grid_arrays(x0_grid_between, x1_grid))\n",
    "    # between the x1 points\n",
    "    x_test_arrays.append(arrays_to_grid_arrays(x0_grid, x1_grid_between))\n",
    "    # between x0 and x1 points\n",
    "    x_test_arrays.append(arrays_to_grid_arrays(x0_grid_between, x1_grid_between))\n",
    "\n",
    "    return tuple(np.hstack(x_test_arrays))\n",
    "\n",
    "\n",
    "def split_up_GP_prediction(GP_model, x_test: np.ndarray) -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Divide the GP prediction over smaller batches to keep the CPU and memory usage limited.\"\"\"\n",
    "    GP_mean_arrays, GP_std_arrays = [], []\n",
    "\n",
    "    steps = 100\n",
    "    for i in tqdm(range(steps)):\n",
    "        x_test_part = np.array_split(x_test, steps)[i]\n",
    "        values = GP_model.fast_predict_y(x_test_part)\n",
    "        _GP_mean, _GP_var = [v.numpy().squeeze() for v in values]\n",
    "        _GP_std = np.sqrt(_GP_var)\n",
    "        GP_mean_arrays.append(_GP_mean)\n",
    "        GP_std_arrays.append(_GP_std)\n",
    "\n",
    "    return np.hstack(GP_mean_arrays), np.hstack(GP_std_arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the GP model predictions\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c96cb01874044a9a1cde8fa3c3e62e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the error model predictions\n"
     ]
    }
   ],
   "source": [
    "x0_test, x1_test = get_test_points(*error_model.mean_interpolator.grid)\n",
    "x_test = np.vstack((x0_test, x1_test)).T\n",
    "\n",
    "print(\"Calculating the GP model predictions\")\n",
    "GP_mean, GP_std = split_up_GP_prediction(GP_model, x_test)\n",
    "\n",
    "print(\"Calculating the error model predictions\")\n",
    "em_mean, em_std = error_model.mean_interpolator(x_test), error_model.std_interpolator(x_test)\n",
    "\n",
    "errors_mean = np.abs(GP_mean - em_mean)\n",
    "errors_std = np.abs(GP_std - em_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Biggest errors:\n",
      "\tmean error = 1.1845073390193139e-07\n",
      "\tstd error = 3.0083279783932326e-08\n",
      "Average values:\n",
      "\tmean avg. = 0.004350989080289418\n",
      "\tstd avg. = 0.05397040806698706\n"
     ]
    }
   ],
   "source": [
    "print(\"Biggest errors:\")\n",
    "print(f\"\\tmean error = {errors_mean.max()}\")\n",
    "print(f\"\\tstd error = {errors_std.max()}\")\n",
    "\n",
    "print(\"Average values:\")\n",
    "print(f\"\\tmean avg. = {GP_mean.mean()}\")\n",
    "print(f\"\\tstd avg. = {GP_std.mean()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
