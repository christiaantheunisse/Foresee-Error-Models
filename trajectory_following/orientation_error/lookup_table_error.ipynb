{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate the error introduced by the lookup table for the **angle error**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "### Import the `lib` directory\n",
    "import pathlib\n",
    "import sys\n",
    "\n",
    "# To add the files in the lib directory to the importable packages\n",
    "repo_directory = pathlib.Path().resolve().parents[1]\n",
    "lib_module_dir = str(repo_directory.joinpath(\"lib\"))\n",
    "if lib_module_dir not in sys.path:\n",
    "    sys.path.insert(0, str(repo_directory.joinpath(\"lib\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you pick a training directory from the 2D GP function and not 1D scale function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dir = \"\"\n",
    "\n",
    "assert training_dir != \"\", (\n",
    "    \"You should pick a directory generated by a model training. These are not uploaded to repository so you need to\"\n",
    "    \" generate them yourself by running `angle_error.ipynb`. To save time, you can reduce the number of epochs.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load the **error model**, i.e., **lookup table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from error_model import ErrorModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_models_path = os.path.join(training_dir, \"error_models\")\n",
    "\n",
    "assert os.path.isdir(error_models_path) and os.listdir(error_models_path), \"No error models found\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the error model from epoch 1000...\n",
      "Error model successfully load:\n",
      "<class 'error_model.ErrorModel'>\n",
      "error                    : orientation error\n",
      "param1                   : curvature\n",
      "param2                   : velocity\n",
      "ndim                     : 2\n",
      "x0 boundaries            : (0.0, 1.0)\n",
      "x1 boundaries            : (0.25, 0.5)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "error_model_file = sorted(os.listdir(error_models_path))[-1]\n",
    "error_model_filepath = os.path.join(error_models_path, error_model_file)\n",
    "epoch_str = error_model_file.replace(\"error_model_\", \"\").replace(\".pickle\", \"\")\n",
    "\n",
    "try:\n",
    "    epoch = int(epoch_str)\n",
    "except ValueError:\n",
    "    raise ValueError(f\"A valid epoch number could not be obtained for the error model file at {error_model_filepath}\")\n",
    "\n",
    "print(f\"Loading the error model from epoch {epoch}...\")\n",
    "\n",
    "with open(error_model_filepath, \"rb\") as f:\n",
    "    error_model = pickle.load(f)\n",
    "\n",
    "print(f\"Error model successfully load:\")\n",
    "print(error_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load the **GP**\n",
    "\n",
    "This is a bit of a hacky solution. Make sure to use the same parameters and data used when training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-04 14:19:50.249680: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-09-04 14:19:50.293181: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-09-04 14:19:50.293916: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-04 14:19:50.980988: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from dual_gp_model_SVGP import DualGaussianProcessWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_file = f\"epoch {epoch}.pickle\"\n",
    "params_filepath = os.path.join(training_dir, \"params\", params_file)\n",
    "assert os.path.isfile(params_filepath), f\"Parameters file does not exist for epoch {epoch}. Choose another error model.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dependencies_folder = pathlib.Path().resolve().parent.joinpath(\"dependencies\")\n",
    "VelCurv = pd.read_csv(dependencies_folder.joinpath(\"VelCurv.csv\"))\n",
    "VelCurv_orient_masked = VelCurv[VelCurv[\"mask_orient\"]]\n",
    "x_data = np.vstack((VelCurv_orient_masked[\"curvatures\"].to_numpy(), VelCurv_orient_masked[\"velocities\"].to_numpy())).T\n",
    "y_data = VelCurv_orient_masked[\"orient_errors\"].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params loaded from Training at 2024-08-31 Saturday at 00.36u/params/epoch 1000.pickle...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/christiaan/thesis/foresee-error-models/lib/dual_gp_model_SVGP.py:299: UserWarning: Use the model generated by the `continue_training` constructor only for making predictions.\n",
      "  warnings.warn(\"Use the model generated by the `continue_training` constructor only for making predictions.\")\n",
      "2024-09-04 14:19:54.529474: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-09-04 14:19:54.530740: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1956] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "### Use this model only to do predictions\n",
    "GP_model = DualGaussianProcessWrapper.continue_training(\n",
    "    x_data=x_data,\n",
    "    y_data=y_data,\n",
    "    params_filepath=params_filepath,\n",
    "    use_lognormal=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Find the errors\n",
    "\n",
    "The biggest error is most likely found at the center of each interpolation interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arrays_to_grid_arrays(x0_grid: np.ndarray, x1_grid: np.ndarray) -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Convert two arrays to two arrays that contain all the points on grid spanned by the two input arrays\"\"\"\n",
    "    x0, x1 = np.meshgrid(x0_grid, x1_grid)\n",
    "    return tuple(np.vstack((x0.flatten(), x1.flatten())))\n",
    "\n",
    "\n",
    "def get_test_points(x0_grid: np.ndarray, x1_grid: np.ndarray) -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Get all test points for a 2D grid. Three positions are tested:\n",
    "        1) between the points along parameter x0\n",
    "        2) between the points along parameter x1\n",
    "        3) between the points along parameter x0 and x1\n",
    "    \"\"\"\n",
    "    x0_grid_between, x1_grid_between = x0_grid[:-1] + np.diff(x0_grid) / 2, x1_grid[:-1] + np.diff(x1_grid) / 2\n",
    "\n",
    "    x_test_arrays = []\n",
    "    # between the x0 points\n",
    "    x_test_arrays.append(arrays_to_grid_arrays(x0_grid_between, x1_grid))\n",
    "    # between the x1 points\n",
    "    x_test_arrays.append(arrays_to_grid_arrays(x0_grid, x1_grid_between))\n",
    "    # between x0 and x1 points\n",
    "    x_test_arrays.append(arrays_to_grid_arrays(x0_grid_between, x1_grid_between))\n",
    "\n",
    "    return tuple(np.hstack(x_test_arrays))\n",
    "\n",
    "\n",
    "def split_up_GP_prediction(GP_model, x_test: np.ndarray) -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Divide the GP prediction over smaller batches to keep the CPU and memory usage limited.\"\"\"\n",
    "    GP_mean_arrays, GP_std_arrays = [], []\n",
    "\n",
    "    steps = 100\n",
    "    for i in tqdm(range(steps)):\n",
    "        x_test_part = np.array_split(x_test, steps)[i]\n",
    "        values = GP_model.fast_predict_y(x_test_part)\n",
    "        _GP_mean, _GP_var = [v.numpy().squeeze() for v in values]\n",
    "        _GP_std = np.sqrt(_GP_var)\n",
    "        GP_mean_arrays.append(_GP_mean)\n",
    "        GP_std_arrays.append(_GP_std)\n",
    "\n",
    "    return np.hstack(GP_mean_arrays), np.hstack(GP_std_arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the GP model predictions\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e7fc5b69efa49979f316d8d9e4de33a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the error model predictions\n"
     ]
    }
   ],
   "source": [
    "x0_test, x1_test = get_test_points(*error_model.mean_interpolator.grid)\n",
    "x_test = np.vstack((x0_test, x1_test)).T\n",
    "\n",
    "print(\"Calculating the GP model predictions\")\n",
    "GP_mean, GP_std = split_up_GP_prediction(GP_model, x_test)\n",
    "\n",
    "print(\"Calculating the error model predictions\")\n",
    "em_mean, em_std = error_model.mean_interpolator(x_test), error_model.std_interpolator(x_test)\n",
    "\n",
    "errors_mean = np.abs(GP_mean - em_mean)\n",
    "errors_std = np.abs(GP_std - em_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GP and error models are used to model a lognormal distribution. A better idea of influence of the lookup table error is obtained by calculating the actual value that is as margin in the algorithm. This is done using the following formula:\n",
    "\n",
    "$$\n",
    "    \\epsilon_{margin} = e^{\\mu_{log} + z \\cdot \\sigma_{log}}\n",
    "$$\n",
    "\n",
    "For differents z-scores, the worst case lookup table error $\\epsilon_{lookup\\;table}$ is given in the formula below. Herein are $\\mu_{average}$ and $\\sigma_{average}$ the average values for the mean and standard deviation of the error. $\\mu_{table\\;error}$ and $\\sigma_{table\\;error}$ are the maximum errors caused by the lookup table in the error mean and standard deviation, respectively.\n",
    "$$\n",
    "    \\epsilon_{lookup\\;table} = e^{\\mu_{average} + \\mu_{table\\;error} + z \\cdot (\\sigma_{average} + \\sigma_{table\\;error})} - e^{\\mu_{average} + z \\cdot \\sigma_{average}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Biggest errors:\n",
      "\tmean error = 1.5863826341444565e-07\n",
      "\tstd error = 1.9990993993523887e-07\n",
      "Average values:\n",
      "\tmean avg. = -4.132980444049414\n",
      "\tstd avg. = 1.1669485299062867\n"
     ]
    }
   ],
   "source": [
    "print(\"Biggest errors:\")\n",
    "print(f\"\\tmean error = {errors_mean.max()}\")\n",
    "print(f\"\\tstd error = {errors_std.max()}\")\n",
    "\n",
    "print(\"Average values:\")\n",
    "print(f\"\\tmean avg. = {GP_mean.mean()}\")\n",
    "print(f\"\\tstd avg. = {GP_std.mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lookup table error for different z-values:\n",
      "z-value 0 : 2.543767304435951e-09 rad\n",
      "z-value 1 : 1.846784986742067e-08 rad\n",
      "z-value 2 : 9.239718989917378e-08 rad\n",
      "z-value 3 : 4.0303942672181847e-07 rad\n",
      "z-value 4 : 1.635906072605664e-06 rad\n",
      "z-value 5 : 6.3510456440596386e-06 rad\n",
      "z-value 6 : 2.392195207434611e-05 rad\n",
      "z-value 7 : 8.815247857540953e-05 rad\n",
      "z-value 8 : 0.0003194940672983648 rad\n",
      "z-value 9 : 0.001142978504731218 rad\n"
     ]
    }
   ],
   "source": [
    "mu_average, sigma_average = GP_mean.mean(), GP_std.mean()\n",
    "mu_table_error, sigma_table_error = errors_mean.max(), errors_std.max()\n",
    "\n",
    "print(\"The lookup table error for different z-values:\")\n",
    "for z in range(10):\n",
    "    error = np.exp(mu_average + mu_table_error + z * (sigma_average + sigma_table_error)) - np.exp(\n",
    "        mu_average + z * sigma_average\n",
    "    )\n",
    "    print(f\"z-value {z} : {error} rad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
